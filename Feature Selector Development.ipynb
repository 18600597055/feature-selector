{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Feature Selection Class\n",
    "\n",
    "In this notebook we will develop a Python class for feature detection. The objective is to create a an object that encompasses several different feature selection methods and that can be applied to multiple datasets.Emphasis will be placed on keeping track of the steps used and the different parameters for repeatability. The first implementation will use three feature selection methods\n",
    "\n",
    "1. Remove columns with a missing percentage greater than a specified threshold\n",
    "2. Remove collinear variables with a correlation greater than a specified correlation coefficient\n",
    "3. Remove features with 0.0 importance from a gradient boosting machine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/application_train.csv')\n",
    "train = train.sample(10000)\n",
    "train_labels = train['TARGET']\n",
    "train = train.drop(columns = 'TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "class FeatureSelector():\n",
    "    \"\"\"\n",
    "    Class for performing feature selection for machine learning or data preprocessing.\n",
    "    \n",
    "    Implements five different methods\n",
    "    \n",
    "        1. Remove columns with a missing percentage greater than a specified threshold\n",
    "        2. Remove columns with a single unique value\n",
    "        3. Remove collinear variables with a correlation greater than a specified correlation coefficient\n",
    "        4. Remove features with 0.0 feature importance from a gradient boosting machine (gbm)\n",
    "        5. Remove features that do not contribute to a specified cumulative feature importance from the gbm\n",
    "        \n",
    "    Attributes\n",
    "    --------\n",
    "    \n",
    "    record_missing : dataframe\n",
    "        Records the fraction of missing values for features with missing fraction above threshold\n",
    "    \n",
    "    record_single_unique : dataframe\n",
    "        Records the features that have a single unique value\n",
    "    \n",
    "    record_collinear : dataframe\n",
    "        Records the pairs of collinear variables with a correlation coefficient above the threshold\n",
    "    \n",
    "    record_zero_importance : dataframe\n",
    "        Records the zero importance features in the data according to the gbm\n",
    "    \n",
    "    record_low_importance : dataframe\n",
    "        Records the lowest importance features not needed to reach the threshold of cumulative importance according to the gbm\n",
    "    \n",
    "    feature_importances : dataframe\n",
    "        All the features importances from the gbm\n",
    "    \n",
    "    removal_ops : dict\n",
    "        Dictionary of removal operations and associated features for removal identified\n",
    "        \n",
    "    Notes\n",
    "    --------\n",
    "    \n",
    "        - All 5 operations can be run with the `identify_all` method.\n",
    "        - Calculating the feature importances requires labels (a supervised learning task) \n",
    "          for training the gradient boosting machine\n",
    "        - For the feature importances, the dataframe is first one-hot encoded before training the gbm.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Dataframes recording information about features to remove\n",
    "        self.record_missing = None\n",
    "        self.record_single_unique = None\n",
    "        self.record_collinear = None\n",
    "        self.record_zero_importance = None\n",
    "        self.record_low_importance = None\n",
    "        \n",
    "        self.feature_importances = None\n",
    "        \n",
    "        # Dictionary to hold removal operations\n",
    "        self.removal_ops = {}\n",
    "        \n",
    "        \n",
    "        \n",
    "    def identify_missing(self, data, missing_threshold):\n",
    "        \"\"\"Find the features with a fraction of missing values above `missing_threshold`\"\"\"\n",
    "        \n",
    "        self.missing_threshold = missing_threshold\n",
    "\n",
    "        # Calculate the fraction of missing in each column \n",
    "        missing_series = data.isnull().sum() / data.shape[0]\n",
    "\n",
    "        # Find the columns with a missing percentage above the threshold\n",
    "        record_missing = pd.DataFrame(missing_series[missing_series > missing_threshold]).reset_index().rename(columns = {'index': 'feature', 0: 'missing_fraction'})\n",
    "\n",
    "        to_drop = list(record_missing['feature'])\n",
    "\n",
    "        self.record_missing = record_missing\n",
    "        self.removal_ops['missing'] = to_drop\n",
    "        \n",
    "        print('%d features with greater than %0.2f missing values.\\n' % (len(self.removal_ops['missing']), self.missing_threshold))\n",
    "        \n",
    "    def identify_single_unique(self, data):\n",
    "        \"\"\"Identifies features with only a single unique value. NaNs do not count as a unique value. \"\"\"\n",
    "\n",
    "        # Calculate the unique counts in each column\n",
    "        unique_counts = data.nunique()\n",
    "\n",
    "        # Find the columns with only one unique count\n",
    "        record_single_unique = pd.DataFrame(unique_counts[unique_counts == 1]).reset_index().rename(columns = {'index': 'feature', 0: 'nunique'})\n",
    "\n",
    "        to_drop = list(record_single_unique['feature'])\n",
    "    \n",
    "        self.record_single_unique = record_single_unique\n",
    "        self.removal_ops['single_unique'] = to_drop\n",
    "        \n",
    "        print('%d features with a single unique value.\\n' % len(self.removal_ops['single_unique']))\n",
    "    \n",
    "    def identify_collinear(self, data, correlation_threshold):\n",
    "        \"\"\"\n",
    "        Finds collinear features based on the correlation coefficient between features. \n",
    "        For each pair of features with a correlation coefficient greather than `correlation_threshold`,\n",
    "        only one of the pair is identified for removal. \n",
    "\n",
    "        Using code adapted from: https://gist.github.com/Swarchal/e29a3a1113403710b6850590641f046c\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "\n",
    "        data : dataframe\n",
    "            Data observations in the rows and features in the columns\n",
    "\n",
    "        correlation_threshold : float between 0 and 1\n",
    "            Value of the Pearson correlation cofficient for identifying correlation features\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.correlation_threshold = correlation_threshold\n",
    "\n",
    "        # Calculate the absolute value of the correlations between every column\n",
    "        corr_matrix = data.corr().abs()\n",
    "\n",
    "        # Extract the upper triangle of the correlation matrix\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "\n",
    "        # Select the features with correlations above the threshold\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > correlation_threshold)]\n",
    "\n",
    "        # Dataframe to hold correlated pairs\n",
    "        record_collinear = pd.DataFrame(columns = ['drop_feature', 'corr_feature', 'corr_value'])\n",
    "\n",
    "        # Iterate through the columns to drop\n",
    "        for column in to_drop:\n",
    "\n",
    "            # Find the correlated features\n",
    "            corr_features = list(upper.index[upper[column] > correlation_threshold])\n",
    "\n",
    "            # Find the correlated values\n",
    "            corr_values = list(upper[column][upper[column] > correlation_threshold])\n",
    "            drop_features = [column for _ in range(len(corr_features))]    \n",
    "\n",
    "            # Record the information (need a temp df for now)\n",
    "            temp_df = pd.DataFrame.from_dict({'drop_feature': drop_features,\n",
    "                                             'corr_feature': corr_features,\n",
    "                                             'corr_value': corr_values})\n",
    "\n",
    "            # Add to dataframe\n",
    "            record_collinear = record_collinear.append(temp_df, ignore_index = True)\n",
    "\n",
    "            \n",
    "        self.record_collinear = record_collinear\n",
    "        self.removal_ops['collinear'] = to_drop\n",
    "        \n",
    "        print('%d features with a correlation greater than %0.2f.\\n' % (len(self.removal_ops['collinear']), self.correlation_threshold))\n",
    "\n",
    "    def identify_zero_importance(self, features, labels, eval_metric, task='classification', n_folds=5):\n",
    "        \"\"\"\n",
    "        \n",
    "        Identify the features with zero importance according to a gradient boosting machine.\n",
    "        The gbm is trained with early stopping using a validation set to prevent overfitting. \n",
    "        The feature importances are averaged over n_folds of cross validation to reduce variance. \n",
    "        \n",
    "        Uses the LightGBM implementation (http://lightgbm.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "        Parameters \n",
    "        --------\n",
    "        features : dataframe\n",
    "            Data for training the model with observations in the rows\n",
    "            and features in the columns\n",
    "\n",
    "        labels : array, shape = (1, )\n",
    "            Array of labels for training the model. These can be either binary \n",
    "            (if task is 'classification') or continuous (if task is 'regression')\n",
    "\n",
    "        eval_metric : string\n",
    "            Evaluation metric to use for the gradient boosting machine\n",
    "\n",
    "        task : string, default = 'classification'\n",
    "            The machine learning task, either 'classification' or 'regression'\n",
    "\n",
    "        n_folds : int, default = 5\n",
    "            Number of folds to use for cross validation\n",
    "            \n",
    "        Notes\n",
    "        --------\n",
    "        \n",
    "        - Features are one-hot encoded to handle the categorical variables before training.\n",
    "        - The gbm is not optimized for any particular task and might need some hyperparameter tuning\n",
    "        - Feature importances, including zero importance features, can change across runs\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # One hot encoding\n",
    "        features = pd.get_dummies(features)\n",
    "\n",
    "        # Extract feature names\n",
    "        feature_names = list(features.columns)\n",
    "\n",
    "        # Convert to np array\n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels).reshape((-1, ))\n",
    "\n",
    "        # Create the kfold object\n",
    "        k_fold = KFold(n_splits = n_folds, shuffle = True)\n",
    "\n",
    "        # Empty array for feature importances\n",
    "        feature_importance_values = np.zeros(len(feature_names))\n",
    "        \n",
    "        print('Training Gradient Boosting Model\\n')\n",
    "        \n",
    "        # Iterate through each fold\n",
    "        for train_indices, valid_indices in k_fold.split(features):\n",
    "\n",
    "            train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "            valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "\n",
    "            if task == 'classification':\n",
    "                model = lgb.LGBMClassifier(n_estimators=10000, learning_rate = 0.05, verbose = -1)\n",
    "\n",
    "            elif task == 'regression':\n",
    "                model = lgb.LGBMRegressor(n_estimators=10000, learning_rate = 0.05, verbose = -1)\n",
    "\n",
    "            else:\n",
    "                raise ValueError('Task must be either \"classification\" or \"regression\"')\n",
    "\n",
    "            # Train the model with early stopping\n",
    "            model.fit(train_features, train_labels, eval_metric = eval_metric,\n",
    "                      eval_set = [(valid_features, valid_labels)],\n",
    "                      early_stopping_rounds = 100, verbose = -1)\n",
    "\n",
    "            # Record the feature importances\n",
    "            feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "\n",
    "            # Clean up memory\n",
    "            gc.enable()\n",
    "            del model, train_features, train_labels, valid_features, valid_labels\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "        feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "\n",
    "        # Sort features according to importance\n",
    "        feature_importances = feature_importances.sort_values('importance', ascending = False).reset_index(drop = True)\n",
    "\n",
    "        # Normalize the feature importances to add up to one\n",
    "        feature_importances['normalized_importance'] = feature_importances['importance'] / feature_importances['importance'].sum()\n",
    "        feature_importances['cumulative_importance'] = np.cumsum(feature_importances['normalized_importance'])\n",
    "\n",
    "        # Extract the features with zero importance\n",
    "        record_zero_importance = feature_importances[feature_importances['importance'] == 0.0]\n",
    "        \n",
    "        to_drop = list(record_zero_importance['feature'])\n",
    "\n",
    "        self.feature_importances = feature_importances\n",
    "        self.record_zero_importance = record_zero_importance\n",
    "        self.removal_ops['zero_importance'] = to_drop\n",
    "        \n",
    "        print('\\n%d features with zero importance.\\n' % len(self.removal_ops['zero_importance']))\n",
    "    \n",
    "    def identify_low_importance(self, cumulative_importance):\n",
    "        \"\"\"\n",
    "        Finds the lowest importance features not needed to account for `cumulative_importance` \n",
    "        of the feature importance from the gradient boosting machine. As an example, if cumulative\n",
    "        importance is set to 0.95, this will retain only the most important features needed to \n",
    "        reach 95% of the total feature importance. The identified features are those not needed.\n",
    "\n",
    "        Parameters\n",
    "        --------\n",
    "        cumulative_importance : float between 0 and 1\n",
    "            The fraction of cumulative importance to account for \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.cumulative_importance = cumulative_importance\n",
    "        \n",
    "        # The feature importances need to be calculated before running\n",
    "        if self.feature_importances is None:\n",
    "            raise NotFittedError('Feature importances have not yet been determined. Call the `identify_zero_importance` method` first.')\n",
    "            \n",
    "        # Make sure most important features are on top\n",
    "        self.feature_importances = self.feature_importances.sort_values('cumulative_importance')\n",
    "\n",
    "        # Identify the features not needed to reach the cumulative_importance\n",
    "        record_low_importance = self.feature_importances[self.feature_importances['cumulative_importance'] > cumulative_importance]\n",
    "\n",
    "        to_drop = list(record_low_importance['feature'])\n",
    "\n",
    "        self.record_low_importance = record_low_importance\n",
    "        self.removal_ops['low_importance'] = to_drop\n",
    "    \n",
    "        print('%d features that do not contribute to cumulative importance of %0.2f.\\n' % (len(self.removal_ops['low_importance']), self.cumulative_importance))\n",
    "        \n",
    "    def identify_all(self, features, labels, selection_params):\n",
    "        \"\"\"\n",
    "        Use all five of the methods to identify features to remove.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        \n",
    "        features : dataframe\n",
    "            Data for training the model with observations in the rows\n",
    "            and features in the columns\n",
    "\n",
    "        labels : array, shape = (1, )\n",
    "            Array of labels for training the model. These can be either binary \n",
    "            (if task is 'classification') or continuous (if task is 'regression')\n",
    "            \n",
    "        selection_params : dict\n",
    "           Parameters to use in the five feature selection methhods.\n",
    "           Params must contain the keys ['missing_threshold', 'correlation_threshold', 'eval_metric', 'task', 'cumulative_importance']\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Check for all required parameters\n",
    "        for param in ['missing_threshold', 'correlation_threshold', 'eval_metric', 'task', 'cumulative_importance']:\n",
    "            if param not in selection_params.keys():\n",
    "                raise ValueError('%s is a required parameter for this method' % param)\n",
    "        \n",
    "        # Implement each of the five methods\n",
    "        self.identify_missing(features, selection_params['missing_threshold'])\n",
    "        self.identify_single_unique(features)\n",
    "        self.identify_collinear(features, selection_params['correlation_threshold'])\n",
    "        self.identify_zero_importance(features, labels, selection_params['eval_metric'], selection_params['task'])\n",
    "        self.identify_low_importance(selection_params['cumulative_importance'])\n",
    "        \n",
    "        # Find the number of features identified to drop\n",
    "        self.n_identified = len(set(list(chain(*list(self.removal_ops.values())))))\n",
    "        print('%d total features out of %d identified for removal.\\n' % (self.n_identified, pd.get_dummies(features).shape[1]))\n",
    "        \n",
    "    def check_identified(self):\n",
    "        \"\"\"Check the identified features before removal. Returns a set of the unique features identified.\"\"\"\n",
    "        \n",
    "        all_identified = set(list(chain(*list(self.removal_ops.values()))))\n",
    "        print('%d features identified for removal' % len(all_identified))\n",
    "        \n",
    "        return all_identified\n",
    "        \n",
    "    \n",
    "    def remove(self, data, methods):\n",
    "        \"\"\"\n",
    "        Remove the features from the data according to the specified methods.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "            data : dataframe\n",
    "                Dataframe with features to remove\n",
    "            methods : 'all' or list of methods\n",
    "                If methods == 'all', any methods that have identified features will be used\n",
    "                Otherwise, only the specified methods will be used.\n",
    "                Can be one of ['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance']\n",
    "                \n",
    "        Return\n",
    "        --------\n",
    "            data : dataframe\n",
    "                Dataframe with identified features removed\n",
    "                \n",
    "        \n",
    "        Notes \n",
    "        --------\n",
    "            - This first one-hot encodes the categorical variables in accordance with the gradient boosting machine.\n",
    "            - Check the features that will be removed before transforming data!\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        features_to_drop = []\n",
    "        \n",
    "        data = pd.get_dummies(data)\n",
    "        \n",
    "        if methods == 'all':\n",
    "            \n",
    "            print('{} methods have been run'.format(list(self.removal_ops.keys())))\n",
    "            \n",
    "            # Find the unique features to drop\n",
    "            features_to_drop = set(list(chain(*list(self.removal_ops.values()))))\n",
    "            \n",
    "        else:\n",
    "            # Iterate through the specified methods\n",
    "            for method in methods:\n",
    "                # Check to make sure the method has been run\n",
    "                if method not in self.removal_ops.keys():\n",
    "                    raise NotFittedError('%s method has not been run' % method)\n",
    "                    \n",
    "                # Append the features identified for removal\n",
    "                else:\n",
    "                    features_to_drop.append(self.removal_ops[method])\n",
    "        \n",
    "            # Find the unique features to drop\n",
    "            features_to_drop = set(list(chain(*features_to_drop)))\n",
    "            \n",
    "        # Remove the features and return the data\n",
    "        data = data.drop(columns = features_to_drop)\n",
    "        self.removed_features = features_to_drop\n",
    "        \n",
    "        print('Removed %d features' % len(features_to_drop))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FeatureSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 features with greater than 0.90 missing values.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs.identify_missing(train, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 features with a single unique value.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs.identify_single_unique(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 features with a correlation greater than 0.90.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs.identify_collinear(train, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gradient Boosting Model\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[87]\tvalid_0's auc: 0.728212\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid_0's auc: 0.71335\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[180]\tvalid_0's auc: 0.714743\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid_0's auc: 0.710958\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[113]\tvalid_0's auc: 0.713472\n",
      "\n",
      "77 features with zero importance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs.identify_zero_importance(train, train_labels, eval_metric='auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115 features that do not contribute to cumulative importance of 0.99.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs.identify_low_importance(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 features with greater than 0.90 missing values.\n",
      "\n",
      "5 features with a single unique value.\n",
      "\n",
      "33 features with a correlation greater than 0.95.\n",
      "\n",
      "Training Gradient Boosting Model\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[84]\tvalid_0's auc: 0.744051\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[138]\tvalid_0's auc: 0.71164\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[84]\tvalid_0's auc: 0.72413\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[72]\tvalid_0's auc: 0.724548\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[61]\tvalid_0's auc: 0.702404\n",
      "\n",
      "80 features with zero importance.\n",
      "\n",
      "161 features that do not contribute to cumulative importance of 0.95.\n",
      "\n",
      "182 total features out of 237 identified for removal.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs.identify_all(train, train_labels, {'missing_threshold': 0.9, 'correlation_threshold': 0.95, 'eval_metric': 'auc',\n",
    "                                      'task': 'classification', 'cumulative_importance': 0.95})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182 features identified for removal\n"
     ]
    }
   ],
   "source": [
    "features_identified = fs.check_identified()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance'] methods have been run\n",
      "Removed 182 features\n"
     ]
    }
   ],
   "source": [
    "train_removed = fs.remove(train, methods = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_selector import FeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FeatureSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
